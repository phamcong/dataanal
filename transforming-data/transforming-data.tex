
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{transforming-data}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{transforming-data}{%
\subsection{Transforming Data}\label{transforming-data}}

Your goal during the data gathering phase is to record as much working
data about your observations as possible since you never know which
feature is going to end up being the golden one that allows your machine
learning algorithm to succeed. Due to this, there are usually a few
redundant or even poor features in your dataset. Think back to those
long word problems in grade school that were essentially a simple math
question, but came filled with red herrings to throw you off; feeding an
unfiltered soup of features to your machine learning algorithms is
pretty similar to trying to get it to solve those word problems.

To be effective, many machine learning algorithms need the data passed
to them be discerning, discriminating and independent. In this module,
you're going to discover methods to get your data behaving like that
using transformers. This will help improve your own knowledge of your
data, as well as improve your machine learning algorithms' performance.

A transformer is any algorithm you apply to your dataset that changes
either the feature count or feature values, but does not alter the
number of observations. You can use transformers to mung your data as a
pre-processing step to clean it up before it's fed to other algorithms.
Another popular transformer use is that of dimensionality reduction,
where the number of features in your dataset is intelligently reduced to
a subset of the original.

Once you've used a few basic transformers, you will also learn about
some data cleansing techniques that attempt to rectify problematic
observations.

    \hypertarget{principal-component-analysis-pca}{%
\subsubsection{Principal Component Analysis
(PCA)}\label{principal-component-analysis-pca}}

Unsupervised learning aims to discover some type of hidden structure
within your data. Without a label or correct answer to test against,
there is no metric for evaluating unsupervised learning algorithms.
Principal Component Analysis (PCA), a transformation that attempts to
convert your possibly correlated features into a set of linearly
uncorrelated ones, is the first unsupervised learning algorithm you'll
study.

\hypertarget{what-is-principal-component-analysis}{%
\paragraph{What is principal component
analysis}\label{what-is-principal-component-analysis}}

\textbf{PCA falls into the group of dimensionality reduction
algorithms}. \emph{In many real-world datasets and the problems they
represent, you aren't aware of what specifically needs to be measured to
succinctly address the issue driving your data collection. So instead,
you simply record any feature you can derive, usually resulting in a
higher dimensionality than what is truly needed}. This is undesirable,
but it's the only reliable way you know to insure you capture the
relationship modeled in your data.

If you have reason to believe \emph{your question has a simple answer},
or that \emph{the features you've collected are actually many indirect
observations of some inherent source you either cannot or do not know
how to measure}, then \textbf{dimensionality reduction applies to your
needs}.

\textbf{PCA's approach} to \emph{dimensionality reduction} is to
\textbf{derive a set of degrees of freedom that can then be used to
reproduce most of the variability of your data}. Picture one of those
cartoon style telephone poles; once you have a figure in mind, compare
it to this one:

\begin{figure}
\centering
\includegraphics{pic/telephone.png}
\caption{telephone-front-view}
\end{figure}

Your envisioned image probably looked similar. You could have pictured
it from any other viewing angle, for instance, as if you were floating
directly above it looking down:

\begin{figure}
\centering
\includegraphics{pic/telephone-above.png}
\caption{telephone-above-view}
\end{figure}

However you probably didn't, since that view doesn't contain enough
variance, or information to easily be discernible as a telephone pole.
The frontal view, however, does. Looking at a telephone pole or any
other object from various viewing angles gives you more information
about that object. If the view angles are really close to one another,
the information you get from the views ends up being mostly the same,
with a lot of duplicate information. However if you're able to move to a
completely different angle, you can get a lot more information about the
object you're examining. And if you're wise in choose your view angles,
with just a few calculated glimpses of an object, you can build a rather
comprehensive understanding of it. PCA calculates those best view
angles:

\hypertarget{how-does-pca-work}{%
\paragraph{How Does PCA Work?}\label{how-does-pca-work}}

\textbf{PCA is one of the most popular techniques for dimensionality
reduction}, and \emph{we recommend you always start with it when you
have a complex dataset}. \textbf{It models a linear subspace of your
data by capturing its greatest variability}. Stated differently,
\textbf{it accesses your dataset's covariance structure directly using
matrix calculations and eigenvectors to compute the best unique features
that describe your samples}.

An iterative approach to this would \textbf{first find the center of
your data}, \emph{based off its numeric features}. Next, it would
\textbf{search for the direction that has the most variance or widest
spread of values}. \emph{That direction is the principal component
vector, so it is then added to a list}. By \textbf{searching for more
directions of maximal variance that are orthogonal to all previously
computed vectors}, \emph{more principal component can then be added to
the list}. \textbf{This set of vectors form a new feature space that you
can represent your samples with}.

\hypertarget{on-dimensions-features-and-views}{%
\subparagraph{On Dimensions, Features, and
Views}\label{on-dimensions-features-and-views}}

Each sample in your dataset represents an observable phenomenon, such as
an object in the real world. Each feature in your dataset tells you
details about your samples. \textbf{Recall from earlier chapters that
features and views are synonymous terms; this isn't accidental!} Just
like looking at an object from different views gives you more
information about the object, so too does examining a sample from
different features. Similar or correlated features will produce an
``overlapped'' view of your samples, the same way similar views of an
object also overlap.

\textbf{PCA ensures that each newly computed view (feature) is
orthogonal or linearly independent to all previously computed ones,
minimizing these overlaps}. PCA also orders the features by importance,
assuming that the more variance expressed in a feature, the more
important it is. In our telephone pole example, the frontal view had
more variance than the bird's-eye view and so it was preferred by PCA.

\emph{With the \textbf{newly computed features ordered by importance,
dropping the least important features on the list intelligently reduces
the number of dimensions needed to represent your dataset}, with minimal
loss of information}. This has many practical uses, including boiling
off high dimensionality observations to just a few key dimensions for
visualization purposes, being used as a noise removal mechanism, and as
a pre-processing step before sending your data through to other more
processor-intensive algorithms. We'll look at more real life use cases
in the next unit.

\hypertarget{when-should-i-use-pca}{%
\paragraph{When should I use PCA?}\label{when-should-i-use-pca}}

PCA, and in fact all dimesionality reduction methods, have three main
uses: + To handle the clear goal of reducing the dimensionality and thus
complexity of your dataset. + To pre-process your data preparation for
other supervised learning tasks, suchs as regression and classification.
+ To make visualizing your data easier, since we can only preceive three
dimensions simultaneously.

According to Nielson Tetrad Demographics, the group of people who watch
the most movies are people between the ages of 24 through 35. Let's say
you had a list of 100 movies and surveyed 5000 people from within this
demographic, asking them to rate all the movies they've seen on a scale
of 1-10. By having considerably more data samples (5000 people) than
features (100 ordinal movie ratings), you're more likely to avoid the
\href{https://en.wikipedia.org/wiki/Curse_of_dimensionality}{curse of
dimensionality}.

Having collected all that data, even though you asked 100 questions,
what do you think truly is being measured by the survey? Overall, it is
the collective movie preference per person. You could attempt to solve
for this manually in a supervised way, by break down movies into
well-known genres: + Action + Adventure + Comedy + Crime \& Gangster +
Drama + Historical + Horror + Musicals + Science Fiction + War + Western
+ etc.

Being unsupervised, PCA doesn't have access to these genre labels. In
fact, it doesn't have or care for any labels whatsoever. This is
important because it's entirely possible there wasn't a single western
movie in your list of 1000 films, so it would be inappropriate and
strange for PCA to derive a `Western' principal component feature. By
using PCA, rather than you creating categories manually, it discovers
the natural categories that exist in your data. It can find as many of
them as you tell it to, so long as that number is less than the original
number of features you provided, and as long as you have enough samples
to support it. The groups it finds are the principal components, and
they are the best possible, linearly independent combination of features
that you can use to describe your data.

Being unsupervised, PCA doesn't have access to these genre labels. In
fact, it doesn't have or care for any labels whatsoever. This is
important because it's entirely possible there wasn't a single western
movie in your list of 1000 films, so it would be inappropriate and
strange for PCA to derive a `Western' principal component feature. By
using PCA, rather than you creating categories manually, it discovers
the natural categories that exist in your data. It can find as many of
them as you tell it to, so long as that number is less than the original
number of features you provided, and as long as you have enough samples
to support it. The groups it finds are the principal components, and
they are the best possible, linearly independent combination of features
that you can use to describe your data.

One warning is that again, being unsupervised, PCA can't tell you
exactly know what the newly created components or features mean. If
you're interested in how to interpret your principal components, we've
included two sources in the dive deeper section to help out with that
and highly recommend you explore them.

Once you've reduced your dataset's dimensionality using PCA to best
describe its variance and linear structure, you can then transform your
movie questionnaire dataset from its original {[}1000, 100{]}
feature-space into the much more comfortable, principal component space,
such as {[}1000, 10{]}. You can visualize your samples in this new space
using an Andrew's plot, or scatter plot. And finally, you can base the
rest of your analysis on your transformed features, rather than the
original 100 feature dataset.

PCA is a very fast algorithm and helps you vaporizes redundant features,
so when you have a high dimensionality dataset, start by running PCA on
it and then visualizing it. This will better help you understand your
data before continuing.

\hypertarget{projecting-a-shadow}{%
\subparagraph{Projecting a Shadow}\label{projecting-a-shadow}}

By transforming your samples into the feature space created by
discarding under-prioritized features, a lower dimensional
representation of your data, also known as \textbf{shadow} or
\textbf{projection} is formed. In the shadow, some information has been
lost---it has fewer features after all. You can actually visualize how
much information has been lost by taking each sample and moving it to
the nearest spot on the projection feature space. In the following 2D
dataset, the orange line represents the principal component direction,
and the gray line represents the second principal component. The one
that's going to get dropped:

By dropping the gray component above, the goal is to project the 2D
points onto 1D space. Move the original 2D samples to their closest spot
on the line:

Once you've projected all samples to their closest spot on the major
principal component, a shadow, or lower dimensional representation has
been formed:

The summed distances traveled by all moved samples is equal to the total
information lost by the projection. An an ideal situation, this lost
information should be dominated by highly redundant features and random
noise.

\hypertarget{scikit-learn-and-pca}{%
\paragraph{SciKit-Learn and PCA}\label{scikit-learn-and-pca}}

To get started, import PCA from \texttt{sklearn.decomposition} and then
create a new instance of the model setting the
\texttt{n\_components\ parameter} to the number of dimensions you wish
to keep. This value has to be less than or equal to the number of
features in your original dataset, since each computed component is a
linear combination of your original features:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{PCA}
        
        \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Datasets/students.data}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        
        \PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        \PY{n}{pca}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{df}\PY{p}{)}
        \PY{n+nb}{print} \PY{p}{(}\PY{n}{pca}\PY{p}{)}
        
        \PY{n}{T} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{df}\PY{p}{)} 
        
        \PY{n+nb}{print} \PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{shape}\PY{p}{)} \PY{c+c1}{\PYZsh{} (430, 6) \PYZhy{} 430 Student survey responses, 6 questions..}
        \PY{n+nb}{print} \PY{p}{(}\PY{n}{T}\PY{o}{.}\PY{n}{shape}\PY{p}{)} \PY{c+c1}{\PYZsh{} (430, 2) \PYZhy{} 430 Student survey responses, 2 principal components}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
PCA(copy=True, iterated\_power='auto', n\_components=2, random\_state=None,
  svd\_solver='auto', tol=0.0, whiten=False)
(649, 29)
(649, 2)

    \end{Verbatim}

    Once you've fit the model against your dataframe, you can use it to
transform your dataset's observations (or any other observation that
share its feature space) into the newly computed, principal component
feature space with the \texttt{.transform()} method. This transformation
is bidirectional, so you can recover your original feature values using
\texttt{.inverse\_transform()} so long as you don't drop any components.
If even one component was removed, then after performing the inverse
transformation back to the regular feature space, there will be some
signs of information loss proportional to which component was dropped.

There are a few other interesting model attribute that SciKit-Learn
exposes to you after you've trained your PCA model with the
\texttt{.fit()} method:

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{components\_}} These are your principal component
  vectors and are linear combinations of your original features. As
  such, they exist within the feature space of your original dataset.
\item
  \textbf{\texttt{explained\_variance\_}} This is the calculated amount
  of variance which exists in the newly computed principal components.
\item
  \textbf{\texttt{explained\_variance\_ratio\_}} Normalized version of
  \texttt{explained\_variance\_} for when your interest is with
  probabilities.
\end{itemize}

\hypertarget{pca-gotchas}{%
\paragraph{PCA Gotchas!}\label{pca-gotchas}}

To use PCA effectively, you should be aware of its weaknesses. The first
is that \textbf{PCA is sensitive to the scaling of your features}. PCA
maximizes variability based off of variance (the average squared
differences of your samples from the mean), and then projects your
original data on these directions of maximal variance. If your has a
feature with a large variance and others with small variances, PCA will
load on the larger variance feature. Being a linear transformation, it
will rotate and reorient your feature space so it diverts as much of the
variance of the larger-variance feature (and some of the other features'
variances), into the first few principal components.

When it does this, a feature might go from having little impact on your
transformation to totally dominating the first principal component, or
vice versa. \textbf{Standardizing your variables ahead of time is the
way to free your PCA results of such scaling issues}. \emph{The cases
where you should not use standardization are when you know your feature
variables, and thus their importance, need to respect the specific
scaling you've set up}. In such a case, PCA would be used on your raw
data.

PCA is extremely fast, but for very large datasets it might take a while
to train. All of the matrix computations required for inversion,
eigenvalue decomposition, etc. boggle it down. Since most real-world
datasets are typically very large and will have a level of noise in
them, razor-sharp, machine-precision matrix operations aren't always
necessary. If you're willing to sacrifice a bit of accuracy for
computational efficiency, \emph{SciKit-Learn offers a sister algorithm
called \textbf{RandomizedPCA} that applies some approximation techniques
to speed up large-scale matrix computation}. You can use this for your
larger datasets.

\textbf{The last issue to keep in mind is that PCA is a linear
transformation only}! In graphical terms, it can rotate and translate
the feature space of your samples, but will not skew them. \emph{PCA
will only, therefore, be able to capture the underlying linear shapes
and variance within your data and cannot discern any complex, nonlinear
intricacies. For such cases, you will have to make use different
dimensionality reduction algorithms, such as \textbf{Isomap}}.

\hypertarget{knowledge-checks}{%
\paragraph{Knowledge checks}\label{knowledge-checks}}

\hypertarget{review-question-1}{%
\subparagraph{Review Question 1}\label{review-question-1}}

1/1 point (graded)

Please complete the sentence so that it makes the most sense:

Principal component analysis\ldots{}

\begin{itemize}
\tightlist
\item
  Requires you have labeled features to use as a metric for determining
  which features are most important
\item
  Is a dimensionality reduction technique that builds a simpler,
  non-linear projection or `shadow' of your dataset
\item
  Asserts you have more features than samples so you can avoid the curse
  of dimensionality and the matrix math works out
\item
  Ensures each newly computed feature is orthogonal to all previously
  computed ones, minimizing overlaps
\end{itemize}

\emph{Answer:} \textbf{D} \#\#\#\#\# Review Question 2 1 point possible
(graded)

Which of these statements is wrong?

\begin{itemize}
\tightlist
\item
  PCA can be used to discover the underlying features being assessed by
  a dataset
\item
  The results of PCA depend on the scaling of your data, so having a
  feature with units of `light-years' and another feature with units of
  `GHz' may be disastrous
\item
  When applied to non-linear data, PCA generally isn't as effective as
  when applied to linear data
\item
  Since PCA is sensitive to feature scaling, if you have a feature that
  is a linear transformation of the other, e.g.~feature2 = 10 *
  feature1, then both features will be ignored
\end{itemize}

\emph{Answer:} \textbf{D}

    \hypertarget{pca-lab}{%
\subsubsection{PCA Lab}\label{pca-lab}}

\hypertarget{assignment-1}{%
\paragraph{Assignment 1}\label{assignment-1}}

\hypertarget{lab-assignement-1}{%
\subparagraph{Lab Assignement 1}\label{lab-assignement-1}}

In this assignment, you're going to experiment with a real life
armadillo sculpture scanned using a Cyberware 3030 MS 3D scanner at
Stanford University. The sculpture is available as part of their 3D
Scanning Repository, and is a very dense 3D mesh consisting of 172974
vertices! The mesh is available for you, located at
/Module4/Datasets/\textbf{stanford\_armadillo.ply}. It is \emph{not} a
Python file, so don't attempt to load it with a text editor!

Open up the Module4/\textbf{assignment1.py} starter code and read
through it carefully. You will notice the use of a new library, Plyfile.
This library loads up the 3D binary mesh for you. The mesh is further
converted into a Pandas dataframe for your ease of manipulation.
Complete the following tasks:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Before changing any of the code, go ahead and execute assignment1.py.
  You should see the 3D armadillo. Your goal is to reduce its
  dimensionality from three to two using PCA to cast a shadow of the
  data onto its two most important principal components. Then render the
  resulting 2D scatter plot.
\item
  Fill out the proper code in the \texttt{do\_PCA()} and
  \texttt{do\_RandomizedPCA()} methods. Be sure to \textbf{return} the
  result of your transformation! You may even want to read the
  SciKit-Learn documentation on
  \href{http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\#sklearn.decomposition.PCA.transform}{\texttt{.transform()}},
  just for future reference so you know what data type comes out of it.
\item
  Re-run the application! Then, answer the questions below:
\end{enumerate}

\hypertarget{lab-question-1}{%
\subparagraph{Lab Question 1}\label{lab-question-1}}

1 point possible (graded)

The first time you see the armadillo in 3D, what direction was its face
pointing towards?

\begin{itemize}
\tightlist
\item
  Left, Towards the negative X-Axis
\item
  Up, Towards the positive Z-Axis
\item
  Right, Towards the positive X-Axis
\item
  Down, Towards the negative Z-Axis
\end{itemize}

\emph{Answer:} \textbf{A} \#\#\#\#\# Lab Question 2 2 point possible
(graded)

Were you able to discern any \textbf{visual} differences between the
transformed PCA results and the transformed RandomizedPCA results?

\begin{itemize}
\tightlist
\item
  Yes, the RandomizedPCA version was no longer even recognizable as an
  armadillo
\item
  Yes, the RandomizedPCA version was a lot less true to the original
  than the regular PCA version
\item
  Yes, but it wasn't a lot\ldots{} just minor differences
\item
  No, they pretty much looked the same to me
\end{itemize}

Which executed faster, RandomizedPCA or PCA?

\begin{itemize}
\tightlist
\item
  PCA
\item
  RandomizedPCA
\end{itemize}

\emph{Answer:} \textbf{RandomizedPCA}

\hypertarget{lab-assignement-2}{%
\subparagraph{Lab Assignement 2}\label{lab-assignement-2}}

In Lab Assignment 1, you applied PCA to a dataset generated by
3D-scanning an actual sculpture. Real life 3D objects are a good segue
to PCA, since it's \textbf{fun} to see its effects on a dataset we can
see and touch. Another benefit is that all three spatial dimensions,
\textbf{x}, \textbf{y}, and \textbf{z}, each measure the same
unit-length relative to one another, so no extra consideration need be
made to account for PCA's weakness of requiring feature scaling.

But now the \textbf{fun} is over. Gaining some practical experience with
real-world datasets, which rarely allot you the luxury of having
features all on the same scale, will help you see how critical feature
scaling is to PCA. In this lab, you're going to experiment with a subset
of UCI's Chronic Kidney Disease data set, a collection of samples taken
from patients in India over a two month period, some of whom were in the
early stages of the disease. The starter code over at
/Module4/\textbf{assignment2.py}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Start by looking through the attribute information on the dataset
  website. Whenever you're given a dataset, the first thing you should
  do is find out as much about it as possible, both by reading up on any
  metadata, as well as by prodding through the actual data.
  Particularly, pay attention to what the docs say about these three
  variables: \textbf{bgr}, \textbf{rc}, and \textbf{wc}.
\item
  Load up the \textbf{\texttt{kidney\_disease.csv}} dataset from the
  /Module4/Datasets/ directory, and drop \textbf{all rows} that have
  \emph{any} nans. You're probably already a pro at doing that by now.
  In addition to getting rid of nans, did you know that the
  \texttt{.dropna()} method (upon completion) also automatically
  re-checks your features and assigns them an appropriate inferred data
  types?
\item
  Use an appropriate indexer command to select only the following
  columns: \textbf{bgr}, \textbf{rc}, and \textbf{wc}. Or alternatively,
  you can drop every other column, but it's probably easier to just use
  an indexer to select the one's you wish to keep.
\item
  Do a check of your dataframe's dtypes. Anything that didn't make it to
  the right type, you may want to investigate. Look through the data and
  identify \emph{why} the conversion failed. These types of problems
  often arise when you aren't in control of how your data is organized.
  Luckily the issue isn't too bad so once you've identified it, you can
  fix it through simple numeric coercion.
\item
  Print the variance of your dataset, as well as a \texttt{.describe()}
  printout.
\item
  Reduce your dataset to two principal components by run it through PCA,
  then check out the resulting visualization.
\end{enumerate}

\hypertarget{lab-question-1-1}{%
\subparagraph{Lab Question 1}\label{lab-question-1-1}}

1 point possible (graded)

The first time you see the armadillo in 3D, what direction was its face
pointing towards?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\tightlist
\item
  Serum Creatinine (numerical) sc in mgs/dl
\item
  Serum (numerical) sod in mEg/L
\item
  Potassium (numerical) pot in mEg/L
\item
  Hemoglobin (numerical) hemo in gms
\end{enumerate}

Having reviewed the dataset metadata on
\href{https://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease}{its
website}, what are the units of the \textbf{wc}, White Blood Cell Count
feature? An example of where units are defined is shown above. NOTE: In
case the UCI site is down,
\href{http://mlr.cs.umass.edu/ml/datasets/Chronic_Kidney_Disease}{here
is a mirror}.

\begin{itemize}
\tightlist
\item
  mm/Hg
\item
  mgs/dl
\item
  mEq/L
\item
  cells/cumm
\item
  gms
\end{itemize}

\emph{Answer:} ** **

\hypertarget{lab-question-2}{%
\subparagraph{Lab Question 2}\label{lab-question-2}}

2 point possible (graded)

Why did the \texttt{.dropna()} method fail to convert all of the columns
to an appropriate numeric format?

\begin{itemize}
\item
  It actually did successfully convert them
\item
  There were a few erroneous leading tab / whitespace characters
\item
  There were a few erroneous trailing tab / whitespace characters
\item
  The dataset comma offset was incorrect in a few rows causing nans to
  move into the next column
\item
  Yes, the RandomizedPCA version was no longer even recognizable as an
  armadillo
\item
  Yes, the RandomizedPCA version was a lot less true to the original
  than the regular PCA version
\item
  Yes, but it wasn't a lot\ldots{} just minor differences
\item
  No, they pretty much looked the same to me
\end{itemize}

Sort the features below from the largest to the smallest variance
amount.

\begin{itemize}
\tightlist
\item
  bgr, rc, wc
\item
  bgr, wc, rc
\item
  rc, wc, bgr
\item
  rc, bgr, wc
\item
  wc, bgr, rc
\item
  wc, rc, bgr
\end{itemize}

\emph{Answer:} ** **

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

You're almost there! The last thing you have to do, and the purpose of
this lab really, is to see how feature scaling alters your PCA results.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Make a backup of your \textbf{assignment2.py} file for safe keeping.
\item
  Change the line that reads: \texttt{python\ scaleFeatures\ =\ False}
  So that is now reads: \texttt{python\ scaleFeatures\ =\ False}.
\item
  Also take a look inside of \textbf{assignment2\_helper.py}. There are
  some \emph{important} notes in there about what SKLearn's \emph{star
  transform()} methods do, and why they do it. You will need to know
  this information for future labs!
\item
  Re-run your assignment and then answer the questions below:
\end{enumerate}

\emph{Answer:} ** **

\hypertarget{lab-questions-continued}{%
\subparagraph{Lab Questions (Continued)}\label{lab-questions-continued}}

2 points possible (graded)

Did scaling your features affect their variances at all? + Yes + No

\emph{Answer:} ** **

After scaling your features, are the green patients without chronic
kidney disease more cleanly separable from the red patients with chronic
kidney disease?

\begin{itemize}
\tightlist
\item
  They are less separable
\item
  There isn't much change
\item
  They are more separable
\end{itemize}

\emph{Answer:} ** **

    \hypertarget{isomap}{%
\subsubsection{Isomap}\label{isomap}}

Similar to PCA, \textbf{Isomap is also an unsupervised learning
technique that reduces the dimensionality of your dataset. No labels or
classifications are needed to guide it except your raw data}. You can
literally apply Isomap to any dataset during your exploratory analysis.
In fact \emph{you should transform your high dimensionality datasets
with Isomap when they do not exhibit the behavior you want, having been
passed through PCA}.

\textbf{PCA is faster than Isomap and works well in most situations, but
its limitation is that it assumes a linear relationship exist between
your features}. What happens when your data has a non-linear structure?
Take for instance a set of images produced by photographing the rotation
one of those fancy, \$1500 office chairs about its Y-axis. From a
stationary camera's perspective, the rotation is a non-linear
transformation, not well suited at all for PCA:

\begin{longtable}[]{@{}c@{}}
\toprule
\begin{minipage}[b]{0.97\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.97\columnwidth}\centering
\emph{Authman's Commando Chair: Non-linear, and linear transformation
examples\ldots{}}\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

In reality though, just by spinning the chair, you're really only
altering a single degree of freedom. Dimensionality reduction aims to
derive a set of degrees of freedom that can then be used to reproduce a
lower dimensional embedding of your data, so a non-linear reduction
algorithm should be able to recognize that within this image space, the
sequence of pictures lie along a single-dimensional, continuous curve.

On the other hand, if the camera were translated directly up and down
rather than rotating the chair itself, then that would be an example of
a linear transformation from the camera's perspective, and PCA and other
linear reduction techniques would be better suited for reducing the
dimensionality of the resulting dataset:

\begin{longtable}[]{@{}c@{}}
\toprule
\begin{minipage}[b]{0.97\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.97\columnwidth}\centering
\emph{Authman's Commando Chair: linear transformation examples (the
camera were translated directly up and down rather than rotating)}\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

In order to address non-linear situations, \textbf{Isomap uses an
entirely different approach to the dimensionality reduction problem}.
One that is highly efficient, albeit more processor intensive than PCA.
Nonetheless, for non-linear relationships it is a must. \textbf{Its
goal: to uncover the intrinsic, geometric-nature of your dataset, as
opposed to simply capturing your datasets most variant directions}.

\hypertarget{how-does-isomap-work}{%
\paragraph{How Does Isomap Work?}\label{how-does-isomap-work}}

Isomap operates by first computing each record's nearest neighbors. This
is done by comparing each sample to every other sample in the dataset.
Only a sample's K-nearest samples qualify for being included in its
nearest-neighborhood samples list. A neighborhood graph is then
constructed by linking each sample to its K-nearest neighbors. The
result is similar to map of roads that is traversed in order to move
from point to point.

\begin{longtable}[]{@{}c@{}}
\toprule
\begin{minipage}[b]{0.97\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.97\columnwidth}\centering
\emph{A comparison by \href{http://web.mit.edu/cocosci/josh.html}{Josh
Tenenbaum} of the direct Euclidean vs manifold neighborhood path
distances of the classic `swiss-roll', as appears in
\href{http://web.mit.edu/cocosci/Papers/sci_reprint.pdf}{Science 290
(5500), December 2000}.}\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

You've already encountered one form of multi-dimensional scaling. Do you
recall what it was? It was actually none other than PCA itself! Multi
dimensional scaling is a process of taking samples in N-dimensional
space, and representing them on some other M-dimensional space, while
attempting to preserve the inter-sample distances as much as possible.
If you're going from a lower to higher dimensionality, then the
distances can be preserved perfectly. But when you reduce dimensions, as
you've seen some information is lost.

Fun fact, the actual distance formula used to calculate the K-nearest
samples doesn't even have to be 100\% precise. As long as it works
reasonably well for `close by' samples. The truth of the matter is that
with Isomap, samples far away from one another aren't included in each
other's neighborhood map, and therefore do not affect the final
simplified manifold produced by multi dimensional scaling. Due to this,
some very nice speed optimizations can be enacted with the distance
calculations.

\hypertarget{when-should-i-use-isomap}{%
\paragraph{When Should I Use Isomap?}\label{when-should-i-use-isomap}}

Isomap has so many real-world use cases. The most straightforward of
which being whenever you believe a simpler surface can describe and
preserve the inherent structure of your higher dimensionality dataset,
and want to retrieve that simpler embedding.

Isomap is better than linear methods when dealing with almost all types
of real image and motion tracking. Recalling the expensive office chair
example from earlier, either rotating the chair, having the camera zoom
into the chair, or panning around the chair, etc. are all examples of
very natural, non-linear motions we undertake often. If you're dataset
comes from images captured in a natural way, or your data is
characterized by similar types of motions, consider using isomap. Simply
translating the camera up and down, or striding it left and right, are
linear motions that aren't as common unless you're doing
\href{https://www.youtube.com/watch?v=x4eLsRUbtBk}{this kind} of
movement.

Isomap can also be better than linear methods at estimating a more
accurate distance metric between different observations of your samples.
Consider the following image sequence of an air dancer being deformed in
the wind:

Even trained against the full set of images (plus any intermediary
images), a linear model would calculate the distance between the first
and last pictures using a straight-line, high dimensionality, Euclidean
metric. This of course doesn't take into account the manifold's
geometry. Isomap would do a better job by traversing the nearest
neighborhood map:

Another example of this would be trying to get from one spot on a sphere
to another; the Euclidean distance would be the shortest path connecting
the two locations, namely, a straight line. The manifold path on the
other hand, would do a much better job sticking to the surface of the
sphere, based on how far apart the neighborhood sampling is. Note: In
the above graph, the orange isomap plot as been interpolated; a more
accurate representation would be polyline segments connecting the four
points in order.

So long as the underlying relationship is non-linear, another area
isomap tends excel at is the grouping and identifying of similar
variations in similar data samples. Due to this, it is extremely useful
as a preprocessor step before conducting supervised learning tasks, such
as classification or regression. Even the official example of isomap in
SciKit-Learn's documentation is of it being applied to group similar
variations of handwritten digits, and outperforming PCA while doing so!

\begin{longtable}[]{@{}c@{}}
\toprule
\tabularnewline
\midrule
\endhead
\emph{73 Pepper Variations From Dr.~Baumler's Garden, Courtesy Denise
Thornton}\tabularnewline
\bottomrule
\end{longtable}

Finally, isomap's benefits also include all the other reasons you would
use PCA or any other dimensionality reduction technique, including
visualization and data compression.

\hypertarget{scikit-learn-and-isomap}{%
\paragraph{SciKit-Learn and Isomap}\label{scikit-learn-and-isomap}}

Isomap is a method of SciKit-Learn's manifold module:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{manifold}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{iso} \PY{o}{=} \PY{n}{manifold}\PY{o}{.}\PY{n}{Isomap}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{iso}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{df}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:} Isomap(eigen\_solver='auto', max\_iter=None, n\_components=2, n\_jobs=1,
            n\_neighbors=4, neighbors\_algorithm='auto', path\_method='auto', tol=0)
\end{Verbatim}
        
    As with PCA, \textbf{n\_components} is the number of features you want
your dataset projected onto, and n\_neighbors defines the neighborhood
size used to create the node neighborhood map. Be sure to experiment
with \textbf{n\_neighbors} particularly, as your neighborhood size will
directly effect how the manifold mapping is generated:

The \emph{larger} your \textbf{n\_neighbors} value is, the \emph{longer}
it will take to calculate the node neighborhood map. In the above
animation, the neighborhood sizes in order are: 2, 3, 4, 5, 6, 7, 8, 16,
32, 64. Notice that at 64 connectivity, the manifold almost looks
similar to PCA. You will have to experiment with the neighborhood
connectivity in order to get the best results.

That said, in addition to considering how many neighbors each sample
should have, you also need to reflect on how many samples realistically
even need to be collected in order for you to properly capture your
lower dimensional manifold! A rule of thumb is the curvier your dataset
is, and by curvier we mean the sharpness of the edges, the more dense
your samples must be in order to capture its latent relationships:

With your Isomap instance created, you can calculate your new feature
set the same way you did with PCA using \texttt{fit()}. Fitting the data
just calculates the basis of the lower dimensional encoding. To actually
project your data into that space, transforming it by calling:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n}{manifold} \PY{o}{=} \PY{n}{iso}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{df}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{df shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{df}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{manifold shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{manifold}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
df shape:  (649, 29)
manifold shape:  (649, 2)

    \end{Verbatim}

    Unlike PCA, Isomap transformations are unidirectional so you will not be
able to \texttt{.inverse\_transform()}your projected data back into your
original feature space, even if it has the same number of dimensions as
your original dataset.

\textbf{Running Isomap is a lot slower than PCA} since a lot more is
happening under the hood, particularly \textbf{for large n\_neighbors
values}, but it provides a simple way to analyze and manipulate high
dimensional samples in terms of its intrinsic nonlinear degrees of
freedom. This is because \textbf{isomap attempts to keep the global
structure of your data as it reduces its dimensionality}. You can use it
in any case where nonlinear geometry degrades the effectiveness of PCA.

\textbf{Isomap is also a bit more sensitive to noise than PCA}. Noisy
data can actually act as a conduit to short-circuit the nearest
neighborhood map, cause isomap to prefer the `noisy' but shorter path
between samples that lie on the real geodesic surface of your data that
would otherwise be well separated.

When using unsupervised dimensionality reduction techniques, be sure to
use the feature scaling on all of your features because the
nearest-neighbor search that Isomap bases your manifold on will do
poorly if you don't, and PCA will prefer features with larger variances.
As explained within the labs' source code, SciKit-Learn's
\textbf{StandardScaler} is a good-fit for taking care of scaling your
data before performing dimensionality reduction.

\hypertarget{knowledge-checks}{%
\paragraph{Knowledge Checks}\label{knowledge-checks}}

\hypertarget{review-question-1}{%
\subparagraph{Review Question 1}\label{review-question-1}}

1/1 point (graded)

\emph{Which of the following explanations of isomap is true?}

\begin{itemize}
\item
  Isomap can be used as a powerful noise removal tool, since a smooth
  manifold is created by ``short circuiting'' the nearest neighbor map
  when calculating distances
\item
  Isomap is usually faster than PCA because it's quicker to compute a
  nearest neighbor map than to do matrix decomposition
\item
  \textbf{A one sentence summary of isomap's implementation is that at
  its core, it is essentially a node distance map that has been fed into
  a special type of PCA correct}
\item
  Isomap will not function without a completely accurate distance
  metric, since it needs to know the precise distance to every single
  sample, including distant ones
\end{itemize}

\hypertarget{review-question-2}{%
\subparagraph{Review Question 2}\label{review-question-2}}

1/1 point (graded)

\emph{Isomap is most beneficial\ldots{}}

\begin{itemize}
\tightlist
\item
  When your data lacks an inherent manifold
\item
  \textbf{When a non-linear, geometric structure is expressed in your
  data correct}
\item
  When you are uncertain how many samples are needed to capture the
  underlying nature of your data
\item
  When your high dimensionality data has a hidden, linear relationship
  expressed within it
\end{itemize}

\hypertarget{assignment-4}{%
\paragraph{Assignment 4}\label{assignment-4}}

\hypertarget{lab-assignment-4}{%
\subparagraph{Lab Assignment 4}\label{lab-assignment-4}}

After having a brief conversation with
\href{https://en.wikipedia.org/wiki/Joshua_Tenenbaum}{Joshua Tenenbaum},
the primary creator of the isometric feature mapping algorithm, it only
seems right that we make your first lab assignment be replicating his
canonical, dimensionality reduction research experiment for visual
perception! In fact, you will also be using his
\href{https://github.com/authman/DAT210x/blob/master/Cached\%20Datasets/face_data.zip}{original
dataset}
(\href{https://web.archive.org/web/20160913051505/http://isomap.stanford.edu/datasets.html}{cached
website}{]}) from December 2000. It consists of 698 samples of
4096-dimensional vectors. These vectors are the coded brightness values
of 64x64-pixel heads that have been rendered facing various directions
and lighted from many angles. Replicate Dr.~Tenenbaum's experiment by:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Applying both PCA and Isomap to the 698 raw images to derive 2D
  principal components and a 2D embedding of the data's intrinsic
  geometric structure.
\item
  Project both onto a 2D scatter plot, with a few superimposed face
  images on the associated samples.
\item
  Extra: If you're feeling fancy, increase n\_components to three, and
  plot your scatter plot on a 3D chart.
\end{enumerate}

\textbf{NOTE:} If you encounter issues with loading .mat files using
SciPy, you might want to see this
\href{http://stackoverflow.com/questions/35283073/scipy-io-loadmat-doesnt-work}{Stack
Overflow} post and check the version of SciPy you're using.

\hypertarget{lab-questions}{%
\subparagraph{Lab Questions}\label{lab-questions}}

4 points possible (graded) (option in bold text is the answer)

Between linear PCA and the non-linear Isomap, which algorithm is better
able to capture the true nature of the faces dataset when reduced to two
component?

\begin{itemize}
\tightlist
\item
  PCA
\item
  \textbf{IsoMap}
\end{itemize}

Each coordinate axis of your 3D manifold should correlate highly with
one degree of freedom from the original, underlying data. In the isomap
plot of the first two components (0 and 1), which `\textbf{degree of
freedom}' do you think was encoded onto first component (the X-axis)
encoded? In other words, what varies as you move horizontally in your
manifold rendering?

\begin{itemize}
\tightlist
\item
  \textbf{Left and Right Head Position}
\item
  Down and Up Head Position
\item
  Lighting Position
\item
  Something else
\end{itemize}

Alter your code to graph the second and third components (index=1 and 2)
instead of the 0th and 1st, for both PCA and Isomap. Look \emph{closely}
at the Isomap plot. Can you tell what `degree of freedom' the X axis
represents?

\begin{itemize}
\tightlist
\item
  Left and Right Head Position
\item
  \textbf{Down and Up Head Position}
\item
  Lighting Position
\item
  Something else
\end{itemize}

In his experiment, Dr.~Tenenbaum set his K-parameter (n\_neighbors is
SciKit-Learn) to 8. Try reducing that figure down to 3 and re-running
your code. Does the X-Axis still represent the same degree of freedom?

\begin{itemize}
\tightlist
\item
  \textbf{Yes}
\item
  No
\end{itemize}

\hypertarget{assignment-5}{%
\subsubsection{Assignment 5}\label{assignment-5}}

\hypertarget{lab-assigment-5}{%
\paragraph{Lab Assigment 5}\label{lab-assigment-5}}

Now that you've had your first taste of isomap, let's take your
knowledge of it to the next level.

Whatever your high-dimensional samples are, be they images, sound files,
or thoughtfully collected attributes, they can all be considered single
points in a high dimensional feature-space. Each one of your
observations is just a single point. Even with a high dimensionality,
it's possible that most or all your samples actually lie on a lower
dimension surface. Isomap aims to capture that embedding, which is
essentially the motion in the underlying, non-linear degrees of freedom.

By testing isomap on a carefully constructed dataset, you will be able
to visually confirm its effectiveness, and gain a deeper understanding
of how and why each parameter acts the way it does. The \textbf{ALOI},
\href{http://aloi.science.uva.nl/}{Amsterdam Library of Object Images},
hosts a huge collection of 1000 small objects that were photographed in
such a controlled environment, by systematically varying the viewing
angle, illumination angle, and illumination color for each object
separately. To really drive home how well isomap does what it claims,
this lab will make use of two image sets taken from the ALOI's
collection.

Manifold extraction, and isomap specifically are really good with vision
recognition problems, speech problems, and many other real-world tasks,
such as identifying similar objects, or objects that have undergone some
change. In the case of the 3D rotating object such as the office chair
example from earlier, if every pixel is a feature, at the end of the
day, the manifold surface is parametrizable by \emph{just} the angle of
the chair---a single feature!

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Start by having a look through the Module4/Datasets/ALOI/ directory.
  There are two directories filled with 192 x 144 pixel images. Identify
  their ordering and try to figure out what's changing between the
  images. They might not be perfectly ordered, but that doesn't matter
  to isomap.
\item
  Create a regular Python list object. Then, write a for-loop that
  iterates over the images in the Module4/Datasets/ALOI/32/ folder,
  appending each of them to your list. Each .PNG image should first be
  loaded into a temporary NDArray, just as shown in the Feature
  Representation reading.

  \emph{Optional: Resample your images down by a factor of two if you
  have a slower computer. You can also convert the image from
  \texttt{0-255} to \texttt{0.0-1.0} if you'd like, but that will have
  no effect on the algorithm's results.}
\item
  Convert the list to a dataframe and run isomap on it to compute the
  lower dimensional embedding. Be sure to set n\_components to 3 so you
  can visualize your manifold. You can also set the neighborhood size to
  six.
\item
  Plot the first two manifold components using a 2D scatter plot, then
  plot the first three components using a 3D scatter plot. Run your
  assignment and then answer the questions below.
\end{enumerate}

\hypertarget{lab-questions-1}{%
\subparagraph{Lab Questions}\label{lab-questions-1}}

2 points possible (graded)

Please describe the results of your isomap embedding--either the 3D or
2D one, it doesn't matter:

\begin{itemize}
\tightlist
\item
  It is completely sporadic. It's hard to detect the pattern at this
  point, since we discarded too many dimensions
\item
  \textbf{The embedding appears to follow an easily traversable, 3D
  spline correct}
\item
  It looks like a geometric pattern, but we probably need to increase
  the neighborhood resolution before a discernible shape emerges
  incorrect
\item
  Isomap rendered the result as a straight line, as expected, since only
  a single degree of freedom is altered in the images
\end{itemize}

\textbf{Explanation}

Encode each image as a single sample. Since SciPy loads images into an
NDArray, rather than converting the individual NDArrays to Pandas
DataFrames and concatenating DataFrames, it makes more sense to just
append all the NDArrays to a list and mass convert the thing in one go.

When rendering the manifold, you should see a smooth curve. In 2D, it
should look like a near perfect circle (check your window axes scale, if
your X-Axis is stretched). In 3D, it should look like the circle as
well, but with the Y-axis values oscillating up and down as if bound to
a sine curve:

Try reducing the `n\_neighbors' parameter one value at a time. Keep
re-running your assignment until the results look visible different.
What is the smallest neighborhood size you can have, while maintaining
similar manifold embedding results?

\begin{itemize}
\tightlist
\item
  1
\item
  \textbf{2}
\item
  3
\item
  4
\item
  5
\item
  6
\end{itemize}

\textbf{Explanation} The correct number is two. The dataset was
generated carefully enough that there really shouldn't be any cause for
noise in the path, unless you introduced it.

\hypertarget{lab-questions-continued}{%
\subparagraph{Lab Questions (Continued)}\label{lab-questions-continued}}

Almost done! Two more steps to complete this lab:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Once you're done answering the first three questions, right before you
  converted your list to a dataframe, add in additional code which also
  appends to your list the images in the Module4/Datasets/ALOI/32\_i
  directory.
\item
  Create a colors Python list. Store a `b' in it for each element you
  load from the /32/ directory, and an `r' for each element you load
  from the `32\_i' directory. Then pass this variable to your 2D and 3D
  scatter plots, as an optional parameter c=colors. Re-run your
  assignment and answer the final question below.
\end{enumerate}

Reset your `n\_neighbors' if you changed it from 6. After adding in the
additional images from the 32\_i dataset, do examine your 2D and 3D
scatter plots again. Have the new samples altered the shape of your
original (blue) manifold?

\begin{itemize}
\tightlist
\item
  No, not in the slightest
\item
  \textbf{Only very slightly\ldots{} correct}
\item
  It looks like a completely different shape
\end{itemize}

What is the arrangement of the newly added, red samples?

\begin{itemize}
\tightlist
\item
  \textbf{Isomap rendered the result as a straight line, intersecting
  the original manifold}
\item
  They are completely sporadic compared to the original manifold, so no
  real pattern exist incorrect
\item
  They are aligned in a smaller circle, intersecting the original
  manifold
\end{itemize}

    \hypertarget{data-cleaning}{%
\subsubsection{Data Cleaning}\label{data-cleaning}}

In \emph{data wrangling}, irrelevant, incomplete, and missing data is
either defaulted to a specific value or removed entirely. NaNs are
stripped out, typographical errors are patched, and perhaps even some
data normalization occurs. The goal of \emph{data cleansing} is to
\emph{take wrangling a step further by rectifying inaccurate and
inconsistent data to standardize it}. Inconsistent data can lead to
false intelligence being produced by your machine learning algorithms,
or no intelligence at all.

Simple data cleansing tasks might be automated and applied out of the
box. More occupation specific tasks require you fully understand the
working environment that generated your raw data. Knowledge of the range
of values you expect to see for a particular feature will help you find
any anomalies that need attention.

A classical example of when cleansing is necessary is when data comes
from multiple sources. If, on average, a specific source consistently
reports figures offset from others, identifying the source of the error,
be it a faulty sensor, or bad reporting, etc., and then making
calculated adjustments is a way to improve your overall data accuracy.
\emph{But without carefully balancing keeping your data as close as
possible to its raw from and making these error corrections, you might
get accused of cooking your data}. After all, \textbf{it's always
possible that there is no error at all}.

\hypertarget{case-study}{%
\subsubsection{Case study}\label{case-study}}

Climate change is a hotly debated topic. Climatologist claim the world
is at a teetering point, and the damages will soon be irreversible if we
don't make swift changes. Opponents claim there is no solid proof that
demonstrates climate change is real, and that the data behind it is
\emph{cooked}, or falsified. Is there any truth to their claim?

The \textbf{NOAA}, National Oceanic and Atmospheric Administration, has
long produced the only spatially complete, long-term (1895-2013) dataset
for climate analyses within the US. In the Climate Divisional Dataset,
each state is represented by 6-10 climate divisions, as shown on the map
below. The \textbf{monthly} temperature readings per climate division
are then calculated by averaging the \textbf{daily} observations from
their ground stations. By using climate divisions and not individual
weather stations, and by using monthly divisional averages for
data-samples, the overall data collection process was eased
considerably. After all, we are talking about government agency that
formed over \href{http://celebrating200years.noaa.gov/}{200 years ago}!
Another added benefit is that less concern need be placed on the
accuracy of specific station measurements, in case of local
inconsistencies. Everything should just get \emph{smoothed over} by
averaging out.

In 2012, the NOAA must have come into better funding because they
exerted an open effort to recalculate their historical climate dataset.
They added in thousands of past observations that had only recently been
digitized (remember when we used to use paper and pencil?), and adjusted
their interpolation formula to remove some \emph{known biases}, such as
jumps introduced by replacing instruments, observation practices such as
differences in readings based on when in the day temperatures are
recorded, inaccuracies caused by station moves, urbanization around the
station, etc. The NOAA further went on to retrospectively adjust their
historic dataset as well, to be more in line with the current
observations at each station, and to make use of the new interpolation
method.

Skeptics were quick to criticize the NOAA's alterations to the historic
dataset, claiming they've manipulated temperature records to create a
warming trend by literally \emph{cooling} the past and \emph{warming}
the present. It seemed that early decades of temperature records were
consistently adjusted downward, and the current, century-long
temperature trend was higher than in the original dataset. Even some of
the historic `hottest days ever' lost their titles.

Part of the data science umbrella of topics include the use of the
scientific method. When experimenting, a control is usually introduced
to reduce variations in the data due to anything except the features
being measured. Knowing how to apply and what type of control to use
typically requires human intervention, as we haven't found a way to get
machines to learn to automatically account for that just yet. There are
different types of controls, for instance negative control or
randomization, which are used depending on the type of experiment being
conducted and the data being gathered. Unfortunately, we only have one
planet to test with and can't go backwards in time. Retrospective
efforts like the NOAA's to polish up their data, will likely continue to
be met with some level skepticism---at least until it gets a lot warmer.

\hypertarget{knowledge-checks}{%
\paragraph{Knowledge Checks}\label{knowledge-checks}}

\hypertarget{review-question}{%
\subparagraph{Review Question}\label{review-question}}

1/1 point (graded)

Given the following options:

\begin{itemize}
\item
  If you encounter errors in your data, don't let anyone know and try to
  hide them by deleting the affected rows, or by cooking your data.
\item
  While gathering data, identify issues that might cause
  inconsistencies, and capture additional features that'll help you
  rectify them.
\item
  cRetrospectively adjust your data to account for discovered problems.
\end{itemize}

Order these options from most desirable to least desirable:

\begin{itemize}
\tightlist
\item
  A, B, C
\item
  A, C, B
\item
  B, A, C
\item
  \textbf{B, C, A}
\item
  C, A, B
\item
  C, B, A
\end{itemize}

\hypertarget{further-reading}{%
\paragraph{Further Reading}\label{further-reading}}

\hypertarget{dive-deeper}{%
\subparagraph{Dive Deeper}\label{dive-deeper}}

Congratulations on making it this far! In the previous module, you
directly explored your data using many visualizations. This time you
learned how to take complex datasets and simplify them using two popular
methods: keeping the most variant set of orthogonal components, and
manifold multi-dimensional scaling of your sample's distance map.

After that you learned about a way errors can creep into your dataset
and potential methods of handling that. Keep all these techniques fresh
in your toolbox by being sure you record some notes about them in the
transforming section of your course map. Remember, there is no hard
order you must stick to while applying these methods. Try something out,
visualize your data, then continue experimenting until you get your
desired results.

Below, as usually, we've included some added details about the
techniques you just studied in case you're interested in further
broadening your knowledge, and taking it to the next level! For
instance, regarding the isomap lab, you learned what each parameter does
and how isomap works; but one thing you might be wondering from our
examples is how interpolation between video frames is performed using
isomap. You know it's impossible to .inverse\_transform() an isomap
manifold, so how in the world might you go about interpolating data in
your original feature space / dataset? Check below!

\hypertarget{pca}{%
\subparagraph{PCA}\label{pca}}

\begin{itemize}
\tightlist
\item
  \href{https://onlinecourses.science.psu.edu/stat505/node/54}{Interpreting
  PCA}
\item
  \href{http://www.stat.cmu.edu/~cshalizi/350/lectures/10/lecture-10.pdf}{Another
  Method for Interpreting PCA}
\item
  \href{http://setosa.io/ev/principal-component-analysis/}{Interactive
  PCA Demo}
\item
  \href{https://stats.stackexchange.com/questions/103953/pca-on-binary-data}{PCA
  on Binary Data}
\item
  \href{http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf}{The
  Best Explanation of the Math Behind PCA You'll Ever Read}
\item
  \href{http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.RandomizedPCA.html}{RandomizedPCA}
\item
  \href{https://stats.stackexchange.com/questions/62677/pca-on-correlation-or-covariance-does-pca-on-correlation-ever-make-sense\#62699}{Correlation
  or Covariance?}
\end{itemize}

\hypertarget{isomap}{%
\subparagraph{Isomap}\label{isomap}}

\begin{itemize}
\tightlist
\item
  \href{http://web.mit.edu/cocosci/Papers/sci_reprint.pdf}{The Historic
  Stanford Isomap Paper, By Josh Tenenbaum}
\item
  \href{https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction}{Lower
  Dimensional Embedding}
\item
  \href{https://www.quora.com/What-is-the-Isomap-algorithm}{Manifold
  Learning}
\item
  \href{http://cs229.stanford.edu/proj2012/ElGhazzalRobaszkiewicz-InterpolatingImagesBetweenVideoFramesUsingNonLinearDimensionalityReduction.pdf}{Interpolating
  Images Between Video Frames Using Non-Linear Dimensionality Reduction}
\item
  \href{}{Cambridge Hand Gesture Data set}
\end{itemize}

\hypertarget{climate-change}{%
\subparagraph{Climate Change}\label{climate-change}}

\begin{itemize}
\tightlist
\item
  \href{https://www.ncdc.noaa.gov/monitoring-references/maps/us-climate-divisions.php}{NOAA
  U.S. Climate Divisional Dataset}
\item
  \href{https://www.ncdc.noaa.gov/temp-and-precip/divisional-comparison/}{Interactive
  Discovery Tool For Comparing Raw Historical Data With the `Enhanced'
  Data} \textbar{} Flash Based
\item
  \href{http://fusion.net/story/341420/the-maddening-world-of-false-equivalence-media-from-a-climate-reporter-who-knows/}{False
  Equivalence} \textbar{} Logic
\item
  \href{https://en.wikipedia.org/wiki/Scientific_control\#Types_of_control}{Control
  Types}
\end{itemize}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
