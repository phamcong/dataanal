{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM and SVC\n",
    "\n",
    "**Support vector machines** are a set of **supervised learning algorithms** that you can use for **classification**, **regression** and **outlier detection purposes**. SciKit-Learn has many classes for SVM usage, depending on your purpose. The one we'll be focusing on is **Support Vector Classifier (SVC)**, but having understood the principles, with a little research, you'll soon be able to use the rest.\n",
    "\n",
    "Suppose you were a student at Coding Dojo. One afternoon on sports day, your instructor challenges you to use any straight surface to separate a mixed group of billiard balls into even and odd sets. \"No problem,\" you say, while laying a pool-stick between the two sets:\n",
    "\n",
    "<img src=\"https://courses.edx.org/asset-v1:Microsoft+DAT210x+4T2016+type@asset+block@pool1.png\" style=\"height: 300px\">\n",
    "\n",
    "Recall, each sample in a dataset is just a point n-dimensional space, each dimension corresponding to a feature. Well, each billiard ball is a \"sample\" that has three features: its number, and it's X and Y-coordinates on the pool table. Stated in data-analysis lingo, you've just created a classifier that can correctly identify samples based on their features. Well done! Having impressed your instructor, she decides to challenge you further by adding more balls onto the table:\n",
    "\n",
    "<img src=\"https://courses.edx.org/asset-v1:Microsoft+DAT210x+4T2016+type@asset+block@pool2.png\" style=\"height: 300px\">\n",
    "\n",
    "\n",
    "Your initial pool-stick placement still works for the most part, but there is probably a better spot to place it that might produce even better separation between the two sets. This is still, no issue for you. You simply adjust the placement of your stick and voilà, it once again works perfectly!\n",
    "\n",
    "<img src=\"https://courses.edx.org/asset-v1:Microsoft+DAT210x+4T2016+type@asset+block@pool3.png\" style=\"height: 300px\">\n",
    "\n",
    "Having moved the stick to a new position, something dawns on you. Where all the balls are on the table doesn't really matter. In fact, even if you had no idea where the 2-ball, the 7-ball, the 10-ball, and the 1-ball, etc. were placed, it wouldn't really make much of a difference in how you placed the stick. The only balls that you really need to be aware of when doing stick-placement, are those balls of either class that are closest to one another. \n",
    "\n",
    "Support vector classifier behaves just like your stick placement. The two things it wishes to fulfill, in order of priority, are first finding a way to separate your data. It does this by looking at the balls from either class that are closest to one another, or in machine learning lingo, the support vectors. The algorithm then ensures it separates your data in the best way possible by orienting the boundary such that the gap, or margin between it and your support vector samples is maximized.\n",
    "\n",
    "To understand why SVC does this, think back to K-Means clustering; do you remember how it was possible for samples in separate groups to actually be closer, or more similar than even samples from the same group, depending on their positioning within the group?\n",
    "\n",
    "<img src=\"https://courses.edx.org/asset-v1:Microsoft+DAT210x+4T2016+type@asset+block@svmknn.png\" style=\"height: 300px\">\n",
    "\n",
    "That means that to classification algorithms, samples closer to the decision boundaries, in the image above, the blue, pink, and green lines, actually seem more similar to one another than samples further away from it. Stated differently, by finding a way to separate the two sets of samples in a manner that **maximizes (? minimizes)** the distance from sample's of either class to the decision boundary, an algorithm will be more certain of the class of each sample. As a result, you're guaranteed to get better classification accuracy.\n",
    "\n",
    "In a nutshell, SVC solves the classification problem by finding the equation of the hyperplane (linear surface) which results in the most separation between two classes of samples. This allows you to confidently label your samples in a very fast and efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Trick 1\n",
    "\n",
    "Let's revisit the dojo billiards ball example. Let's say your instructor notices how awesome you are at separating even and odd samples so decides to throw you a curve, by placing the remaining two balls at very difficult to separate spots on the table:\n",
    "\n",
    "<img src=\"https://courses.edx.org/asset-v1:Microsoft+DAT210x+4T2016+type@asset+block@pool4.png\" style=\"height: 300px\">\n",
    "\n",
    "Your original instructions only were to use any *straight surface* to separate a mixed group of billiard balls into even and odd sets. At this point, it doesn't matter how you orient the pool stick: unless you're next to a black hole where space and time get warped out, you'll never be able to get the flat pool-stick, or any linear structure for that matter, to separate between even and odd featured balls now. So how do you handle the situation? The most mature way possible, of course—by flipping the table! Not out of anger, but a very *calculated* and *coordinated* table-flip that sends all of the balls flying into the air.\n",
    "\n",
    "<img src=\"https://courses.edx.org/asset-v1:Microsoft+DAT210x+4T2016+type@asset+block@flip1.png\" style=\"height: 200px\">\n",
    "\n",
    "<img src=\"https://courses.edx.org/asset-v1:Microsoft+DAT210x+4T2016+type@asset+block@flip2.png\" style=\"height: 200px\">\n",
    "\n",
    "For the split second second all the balls are in the air, you use your coding-ninja skills to slide a large, straight, sheet of paper clean right between even and odd balls. From the perspective of the instructor (who just so happens to be floating directly above the pool table as you can tell from the first few images), it looks like the splitting of the ball samples was done using a curvy line. But from your perspective, looking at the situation from the side of the table and seeing all the balls floating in the air, a very straight, single, linear-sheet of paper was able to split the two sets of balls, properly fulfilling the objective. This trick of yours is **almost** called the kernel trick, used in SVC and many other mathematical algorithms.\n",
    "\n",
    "**The Trick**\n",
    "\n",
    "To be able to place the decision boundary between the billiard balls, you had to get them into the air by transforming them from their original three-dimensional feature-space (table position x, table position y, even/odd status) to a higher, 4D space, where the table position z-coordinate was added. Once the balls were in the air, you could see where the flat paper sheet needed to be slipped between the two types of balls.\n",
    "\n",
    "<img src=\"https://courses.edx.org/asset-v1:Microsoft+DAT210x+4T2016+type@asset+block@savflip.gif\" style=\"height: 350px\">\n",
    "\n",
    "The kernel trick actually uses a nifty geometric shortcut to *skip* your table-flipping, transformation step, and just jump straight to the answer. Without delving too deeply into the math behind it since that's covered in-depth in the Dive Deeper section, the kernel is essentially a similarity function. You give it two samples and it lets you know how similar they are. This is pretty important because in SVC, your first priority is finding out which samples of opposite class are nearest to one another, so that you can position your decision boundary optimally. What the kernel function essentially tells you is, given your current ball positions, how far apart *would the balls have been*, had you flipped the table. This saves you the trouble of cleaning up the balls, and the embarrassment of (possibly) not having the strength to flip the table. In machine learning, some kernels are able to answer the similarity question in an infinite-dimensional space. No machine is capable of flipping that table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Trick 2\n",
    "\n",
    "There are many types of kernels out there, and with SciKit-Learn, you can even define your own. Knowing what type of kernel to use, and under what circumstances to use it, are things that you'll have to discern as a domain expert in the field you are researching. A kernel that performs amazingly for one problem might fail entirely with a different problem. Let's solidify your understanding of kernels through a few examples.\n",
    "\n",
    "Each ball in our billiards dataset has an x-coordinate, a y-coordinate and an even-odd status. Ball 1 and 2 look like this:\n",
    "\n",
    "```python\n",
    "Ball(Index),  XPos,  YPos,  Even\n",
    "1,               7,     4,     0\n",
    "12,              2,     3,     1\n",
    "```\n",
    "\n",
    "In order to be linearly separable, we need to get the balls into the air, a complex transformation. We can model the table-flipping as a function `f(s)`, that takes in a low-dimensional ball sample, and maps it to the higher-dimensional space once the flip has occurred:\n",
    "\n",
    "```python\n",
    "f(s) = (s1*s1,  s1*s2,  s1*s3,\n",
    "        s2*s1,  s2*s2,  s2*s3,\n",
    "        s3*s1,  s3*s2,  s3*s3)\n",
    "```\n",
    "\n",
    "Where s1 is the sample's first feature **XPos**, s2 is the second feature **YPos**, and s3 is the third feature **Even**. This is what your billiard balls look like in the air:\n",
    "\n",
    "```python\n",
    "Ball  s1*s1  s1*s2  s1*s3  s2*s1  s2*s2  s2*s3  s3*s1  s3*s2  s3*s3\n",
    "1        49     28      0     28     16      0      0      0      0\n",
    "12        4      6      2      6      9      3      2      3      1\n",
    "```\n",
    "\n",
    "Things are starting to get hairy and you haven't even slipped the paper between the balls! To do that, you'll have to see how similar each ball vector is. You can use a scalar dot product do to that, defined as:\n",
    "\n",
    "```python\n",
    "<f(a), f(b)>  =  (a.Feature_1 * b.Feature_1)  +\n",
    "                 (a.Feature_2 * b.Feature_2)  + \n",
    "                 ...                          +\n",
    "                 (a.Feature_n * b.Feature_n)\n",
    "```\n",
    "\n",
    "Where a and b are balls. Calculating the dot product between Ball #1 and Ball #12 once they're in the air (in the high-dimensional feature space), the calculation would look like this:\n",
    "\n",
    "```python\n",
    "<f(Ball1), f(Ball12)>  =  49*4  +  28*6  +  0*2  +  28*6  +  16*9  +  0*3  +  0*2  +  0*3  +  0*1\n",
    "<f(Ball1), f(Ball12)>  =  676\n",
    "```\n",
    "\n",
    "Being a ninja, you're not going to have any of that. Rather, using your domain expertise, you derive an interesting kernel function that takes in any two ball samples `(a, b)` in their original, non-flipped, low-dimensional table space of (`XPos`, `YPos`, `Even`), and computes for you the same metric:\n",
    "\n",
    "```python\n",
    "K(a, b)  =  (a.XPos * b.XPos  +  a.YPos * b.YPos  +  a.Even * b.Even) ** 2\n",
    "```\n",
    "\n",
    "Placing `Ball #1` and `Ball #12` into your kernel yields:\n",
    "\n",
    "```python\n",
    "K(Ball1, Ball12)  =  (7*2  +  4*3  +  0*1) ** 2\n",
    "K(Ball1, Ball12)  =  676\n",
    "```\n",
    "\n",
    "Amazing—the same metric, without all the hard work! In [SciKit-Learn](http://scikit-learn.org/stable/modules/svm.html#svm-kernels), the built-in kernels functions provided are essentially K(`Ball1`, `Ball12`). You can also specify a user defined function if you'd like to implement your own kernel function.\n",
    "\n",
    "<img src=\"https://courses.edx.org/asset-v1:Microsoft+DAT210x+4T2016+type@asset+block@svc_visualization.gif\" style=\"height: 350px\">\n",
    "\n",
    "Two different videos of the same data, provided courtesy of Roemer Vlasveld's Blog on [Intro to One-Class SVC](https://rvlasveld.github.io/blog/2013/07/12/introduction-to-one-class-support-vector-machines/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
