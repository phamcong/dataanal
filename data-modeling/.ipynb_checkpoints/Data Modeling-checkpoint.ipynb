{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling\n",
    "\n",
    "### Clustering\n",
    "\n",
    "My nephew wants me to program a video game for him called 'Monster Family', and even provided a description of its characters:\n",
    "\n",
    "<img src='https://courses.edx.org/asset-v1:Microsoft+DAT210x+4T2016+type@asset+block@Haadi.jpg'\n",
    "alt='' style='height:300px;'/>\n",
    "\n",
    "Although he's young, you can tell he's particular about details. To make the game to his likings, I needed to know which monsters were real family members, and which monsters weren't. If he had added just one more column that held that detail, FamilyA, FamilyB, etc., I would have been set:\n",
    "\n",
    "<img src='pic/data-modeling-clustering-1.png'\n",
    "alt='' style='height:200px;'/>\n",
    "\n",
    "This monster 'dataset' is similar to real-world data in that it comes loaded with observational features, but isn't labeled. The one question I want a direct answer to isn't included as a feature. If there were way to automatically group similar samples based solely on their features, we then could use that knowledge to guide us towards actionable intelligence. That way exist, and its called unsupervised clustering.\n",
    "\n",
    "#### Similarity\n",
    "\n",
    "Since the goal of clustering is the grouping of similar records, you have to first define what similarity means. How would you go define monster similarity?\n",
    "\n",
    "<img src='pic/data-modeling-clustering-2.png'\n",
    "alt='' style='height:200px;'/>\n",
    "\n",
    "One way you could group them is by attack power. Perhaps the monsters who deal the most damage to the player belong to a family, the weaker monsters belong to a family, and then the rest grouped as a family too, as shown above.\n",
    "\n",
    "<img src='pic/data-modeling-clustering-3.png'\n",
    "alt='' style='height:200px;'/>\n",
    "\n",
    "Another grouping would be by weakness. It seems a lot of monsters that share a water-based weakness. All of these might belong to the same family. The remaining monsters might each belong to a separate family, or might actually be members of a sporadic family. There are many other ways you could group them as well, such as by size, by name (e.g. monster vs snake), etc. \n",
    "\n",
    "Without a generalizable way to group the samples, deterministic computers can't cluster your data. What's needed is a systematic means of measuring the **overall** similarity between your samples. Let's discuss how that's accomplished in the next section.\n",
    "\n",
    "#### How Does k-Means Work?\n",
    "\n",
    "Clustering groups samples that are similar within the same cluster. The more similar the samples belonging to a cluster group are (and conversely, the more dissimilar samples in separate groups), the better the clustering algorithm has performed. Since clustering is an unsupervised algorithm, this similarity metric must be measured automatically and based solely on your data.\n",
    "\n",
    "The implementation details and definition of *similarity* are what differentiate the many clustering algorithms. The **K-Means** way of doing this, is to iteratively separate your samples into a user-specified number of \"K\" cluster groups of roughly equal variance. Cluster groups are defined by their geometric cluster center, single point referred to as its centroid. Separately, *centroid* and *cluster* are sometimes used interchangeably; but if used together, a cluster is a set of similar samples, and a centroid is just the mean feature-position of all samples assigned to the cluster.\n",
    "\n",
    "The centroids are not records in your dataset, however they do 'exist' within your datasets feature-space. This is important because it allows for a meaningful distance measure to be calculated between the centroids and your samples. Every sample in your dataset is assigned to the centroid nearest to it, so if you have a sample that is 10 units away from CusterA's centroid, and 100 units away from ClusterB's, the sample is assigned to ClusterA.\n",
    "\n",
    "In the case of continuous features, calculating the distance is straightforward. But when you have categorical features, such as 'Cookies n Cream' vs 'Mango' ice cream favors, you'll have to creatively come up with other methods. SciKit-Learn's K-Means implementation only natively supports numeric features types, so we'll leave the discussion on how to do clustering with categorical features to the Dive Deeper section.\n",
    "\n",
    "#### The K-Means Algorithm\n",
    "\n",
    "K-Means starts by placing a user-specified number of \"K\" cluster centers in your feature space. There are many techniques for choosing the first centroid placement, and your results will vary depending on the one you select! The simplest being just use the position of some random samples as the centroids' starting spots.\n",
    "\n",
    "Each cluster then takes ownership of the samples nearest to its centroid, and every sample can only be assigned as single cluster. 'Nearest' is a value that has to be evaluated and in SciKit-Learn, it is defined as the multivariate, n-dimensional Euclidean distance between the sample and the centroid. After this, the centroid location is updated to be the mean value of all samples assigned to it. This mean value is calculated by feature, so the centroid position ends up being a n-length vector within your feature space.\n",
    "\n",
    "The assignment and update steps repeat until there are no more changes in either, at which point the algorithm has converged. K-Means always converges, and it is very fast at doing so. But it does not always converge at the global minima...\n",
    "\n",
    "The technical explanation for what K-Means does is minimizing the within-cluster inertia, or **sum of squared errors** between each sample and its respective centroid. As mentioned, the initial centroid assignment affects the results. Two runs of K-means might produce different outcomes, but the quality of their cluster assignments are ranked by looking at which run has the smallest overall inertia.\n",
    "\n",
    "#### When Should I Use K-Means?\n",
    "\n",
    "Clustering is a natural action we do even as children, by arranging similar shaped blocks and colors. K-Means clustering is best suited when you have a good idea of the number of distinct clusters your unlabeled dataset should be segmented into. Generally, the output of K-Means is used in two ways. To separate your unlabeled data into K groups, which is the clear use case, or to find and use the resulting centroids.\n",
    "\n",
    "##### Separate Your Data\n",
    "\n",
    "Astronomers use clustering to group different star types, classes of planets, and galaxies. Biologists use it to group every living thing by species, genus, and kingdom. In business, clustering is used to segment likely and unlikely prospects, for location assignment, factor endowment, and the assignment and deployment of remote services.\n",
    "\n",
    "##### Centroid Usage\n",
    "\n",
    "Besides divvying up samples, clustering can also provide a layer of abstraction, by directing attention to the cluster and its attributes and not each samples. In the climate change case study from the previous module, you saw how climate divisions were used as a cluster abstraction over individual ground stations for various mentioned reasons. Another example of centroid usage would be a company looking for ideal locations to open a limited number of branches, based on the location of their customers.\n",
    "\n",
    "You can use the centroid to 'compress' your data. By referring to the centroid rather than the data sample, the number of unique values is reduced, which optimizes the execution speed of other algorithms. Isomap, for instance, uses a nearest neighbors algorithm to calculate the distance from the record you want to transform to every sample in the training dataset. By using the record-to-cluster distance approximation in replacement of the individual record-to-sample distances, since there are far fewer clusters than records, you can achieve unprecedented orders of optimization.\n",
    "\n",
    "#### SciKit-Learn and K-Means\n",
    "\n",
    "It's very simple to get up and running with K-Means in SKLearn. Given a dataframe df, you can compute its labels and centroids as follows:\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(df)\n",
    "\n",
    "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=5, n_init=10, n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001, verbose=0)\n",
    "\n",
    "labels = kmeans.predict(df)\n",
    "centroids = kmeans.cluster_centers_\n",
    "```\n",
    "\n",
    "The most important factor for you to focus on being n_clusters, the \"**K**\" number of clusters you want K-Means to place for you. Also experiment with different initialization methods, including rolling your own and in the positions as an NDArray shaped as `[n_clusters, n_features]`. We've include more details for you on that in the dive deeper section.\n",
    "\n",
    "The most important factor for you to focus on being n_clusters, the \"**K**\" number of clusters you want K-Means to place for you. Also experiment with different initialization methods, including rolling your own and in the positions as an NDArray shaped as `[n_clusters, n_features]`. We've include more details for you on that in the dive deeper section.\n",
    "\n",
    "#### K-Means Gotchas!\n",
    "\n",
    "t's easy to understand the K-Means algorithm, and extremely fast to execute. So fast that it's often ran several times over as you saw earlier. Since each successive run of isn't dependent on the results of earlier runs, the execution process lends itself to parallelization, each centroid seeding trial being ran independently. If the clustering job at hand is still taking too long, SciKit-Learn's [MiniBatchKMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html) further optimizes the process for you.\n",
    "\n",
    "Considering how basic of an algorithm it is, K-Means performs pretty well, and its implementation is the basis for a few more advanced clustering algorithms, such as learning vector quantization and Gaussian mixture. Having a solid understanding of K-Means will help you understand those better when you study them.\n",
    "\n",
    "K-Means is only really suitable when you have a good estimate of the number clusters that exist in your unlabeled data. There are many estimation techniques for approximating the correct number of clusters, but you'll have to get that number before running K-Means. Even if you do have the right number of clusters selected, the result produced by K-Means can vary depending on the initial centroid placement. So if you need the same results produced each time, your centroid seeding technique also needs to be able to reliably produce the same placement given the same data. Due to the centroid seed placement having so much of an effect on your clustering outcome, you have to be careful since it is possible to have centroids with only a single sample assigned to them, or even no samples assigned to them in the worst case scenario.\n",
    "\n",
    "Two other key characteristics of K-Means are that it assumes your samples are length normalized, and as such, is sensitive to feature scaling. It also assumes that the cluster sizes are roughly spherical and similar; this way, the nearest centroid is always the correct assignment.\n",
    "\n",
    "#### Knowledge Checks\n",
    "\n",
    "##### Review Question 1\n",
    "\n",
    "*Only one of following statements is true. Which one is it?*\n",
    "\n",
    "+ **Its possible for samples from two different clusters to be more similar to one another than their intra-cluster neighbors, if the the two clusters are large and located near one another correct**\n",
    "+ Real world data typically comes labeled\n",
    "+ Unsupervised clustering aims to group your samples based on their labels\n",
    "+ Centroids are records that live in your dataset and share the same feature space so that a meaningful distance can be calculated between them and your samples\n",
    "\n",
    "##### Review Question 2\n",
    "\n",
    "Once again, only a single one of the following statements is correct. Do you know which one it is?*\n",
    "\n",
    "+ It's possible for a sample to be assigned to two clusters; but only if its equidistant from either cluster.\n",
    "+ The K-Means algorithm scans your dataset to detect clusters using an iterative assignment / update cycle. The algorithm returns the number of clusters found, as well as their centroid position. incorrect\n",
    "+ As a clustering algorithm, K-Means is really only useful for grouping your samples\n",
    "+ **K-Means assumes your features are either length normalized, or that their length encodes a specific meaning.**\n",
    "\n",
    "*Answer*\n",
    "\n",
    "**Incorrect**: \n",
    "\n",
    "Wrong! You have to specify how many clusters exist in your data. Given that number, K-Means will attempt to find *that* many clusters in your data. But the responsibility of specifying the number of clusters is yours, not the algorithm's.\n",
    "\n",
    "**Explanation**\n",
    "\n",
    "A sample can only have a single cluster assignment. Also, you are the one responsible for specifying the number of clusters. K-Means won't tell you the number of clusters in your data. Besides assigning a cluster to your samples, there are many uses for the centroid locations. Review th reading please. Since K-Means cluster assignment depends on an Euclidean length metric, your features have to either be length normalized, or have appropriate units for the algorithm to perform properly.\n",
    "\n",
    "#### Assignment 1\n",
    "\n",
    "##### Lab Assignment 1\n",
    "\n",
    "Many U.S. cities, the U.S. federal government, and even other cities and governments abroad have started subscribing to an Open Data policy, because some data should be transparent and available to everyone to use and republish freely, without restrictions from copyright, patents, or other mechanisms of control. After reading their [terms of use](http://www.cityofchicago.org/city/en/narr/foia/data_disclaimer.html), in this lab you'll be exploring the City of Chicago's Crime data set, which is part of their Open Data initiative.\n",
    "\n",
    "1. Start by navigating over to the [City of Chicago's Crimes dataset](https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2) exploration page. It lists crimes from 2001 to the present, but you'll only be targeting Gambling. The city's website itself has hundreds of other datasets you can browse and do machine learning on.\n",
    "2. Open up the /Module5/**assignment1.py** starter code, and follow the directions to acquire the dataset and properly set it up.\n",
    "3. Fill out the **doKMeans** method to find and plot **seven clusters** and print out their centroids. These could be places a police officer investigates to check for on-going illegal activities.\n",
    "4. Re-run your assignment a few times over, looking at your printed and plotted results. Then answer the following questions.\n",
    "\n",
    "*Note: If Pandas complains about your data, you can use dropna() on any row that has nans in it.*\n",
    "\n",
    "##### Lab Questions\n",
    "\n",
    "2 points possible (graded)\n",
    "\n",
    "You'll notice that the cluster assignments are pretty accurate. Most of them should be spot-on, dead-center. Only one cluster might have been assigned to outliers. Given the results, answer the following questions to the best of your ability:\n",
    "\n",
    "*Did your centroid locations change after you limited the date range to +2011?*\n",
    "\n",
    "+ Their locations are completely different\n",
    "+ **They move slightly...**\n",
    "+ Not at all\n",
    "\n",
    "*What about during successive runs of your assignment? Any centroid location changes happened there?*\n",
    "\n",
    "+ All clusters have moved, and the cluster arrangement isn't anything like it was before\n",
    "+ **All clusters have moved but only slightly, and the centroid arrangement still has the same shape for the most part**\n",
    "+ The clusters did not really move at all, or if they did, it wasn't noticeable\n",
    "+ The cluster centroids are identical according to the print statement output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
