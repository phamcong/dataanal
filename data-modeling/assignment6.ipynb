{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot') # Look Pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you'd like to try this lab with PCA instead of Isomap,\n",
    "# as the dimensionality reduction technique:\n",
    "Test_PCA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot2DBoundary(DTrain, LTrain, DTest, LTest, model):\n",
    "  # The dots are training samples (img not drawn), and the pics are testing samples (images drawn)\n",
    "  # Play around with the K values. This is very controlled dataset so it should be able to get perfect classification on testing entries\n",
    "  # Play with the K for isomap, play with the K for neighbors. \n",
    "\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(111)\n",
    "  ax.set_title('Transformed Boundary, Image Space -> 2D')\n",
    "\n",
    "  padding = 0.1   # Zoom out\n",
    "  resolution = 1  # Don't get too detailed; smaller values (finer rez) will take longer to compute\n",
    "  colors = ['blue','green','orange','red']\n",
    "  \n",
    "\n",
    "  # ------\n",
    "\n",
    "  # Calculate the boundaries of the mesh grid. The mesh grid is\n",
    "  # a standard grid (think graph paper), where each point will be\n",
    "  # sent to the classifier (KNeighbors) to predict what class it\n",
    "  # belongs to. This is why KNeighbors has to be trained against\n",
    "  # 2D data, so we can produce this countour. Once we have the \n",
    "  # label for each point on the grid, we can color it appropriately\n",
    "  # and plot it.\n",
    "  x_min, x_max = DTrain[:, 0].min(), DTrain[:, 0].max()\n",
    "  y_min, y_max = DTrain[:, 1].min(), DTrain[:, 1].max()\n",
    "  x_range = x_max - x_min\n",
    "  y_range = y_max - y_min\n",
    "  x_min -= x_range * padding\n",
    "  y_min -= y_range * padding\n",
    "  x_max += x_range * padding\n",
    "  y_max += y_range * padding\n",
    "\n",
    "  # Using the boundaries, actually make the 2D Grid Matrix:\n",
    "  xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution),\n",
    "                       np.arange(y_min, y_max, resolution))\n",
    "\n",
    "  # What class does the classifier say about each spot on the chart?\n",
    "  # The values stored in the matrix are the predictions of the model\n",
    "  # at said location:\n",
    "  Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "  Z = Z.reshape(xx.shape)\n",
    "\n",
    "  # Plot the mesh grid as a filled contour plot:\n",
    "  plt.contourf(xx, yy, Z, cmap=plt.cm.terrain, z=-100)\n",
    "\n",
    "\n",
    "  # ------\n",
    "\n",
    "  # When plotting the testing images, used to validate if the algorithm\n",
    "  # is functioning correctly, size them as 5% of the overall chart size\n",
    "  x_size = x_range * 0.05\n",
    "  y_size = y_range * 0.05\n",
    "  \n",
    "  # First, plot the images in your TEST dataset\n",
    "  img_num = 0\n",
    "  for index in LTest.index:\n",
    "    # DTest is a regular NDArray, so you'll iterate over that 1 at a time.\n",
    "    x0, y0 = DTest[img_num,0]-x_size/2., DTest[img_num,1]-y_size/2.\n",
    "    x1, y1 = DTest[img_num,0]+x_size/2., DTest[img_num,1]+y_size/2.\n",
    "\n",
    "    # DTest = our images isomap-transformed into 2D. But we still want\n",
    "    # to plot the original image, so we look to the original, untouched\n",
    "    # dataset (at index) to get the pixels:\n",
    "    img = df.iloc[index,:].reshape(num_pixels, num_pixels)\n",
    "    ax.imshow(img, aspect='auto', cmap=plt.cm.gray, interpolation='nearest', zorder=100000, extent=(x0, x1, y0, y1), alpha=0.8)\n",
    "    img_num += 1\n",
    "\n",
    "\n",
    "  # Plot your TRAINING points as well... as points rather than as images\n",
    "  for label in range(len(np.unique(LTrain))):\n",
    "    indices = np.where(LTrain == label)\n",
    "    ax.scatter(DTrain[indices, 0], DTrain[indices, 1], c=colors[label], alpha=0.8, marker='o')\n",
    "\n",
    "  # Plot\n",
    "  plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CongCuongPHAM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# TODO: Use the same code from Module4/assignment4.py to load up the\n",
    "# face_data.mat in a dataset called \"df\". Be sure to calculate the\n",
    "# num_pixels value, and to rotate the images to being right-side-up\n",
    "# instead of sideways. This was demonstrated in the M4/A4 code:\n",
    "#\n",
    "\n",
    "# A .MAT file is a .MATLAB file. The faces dataset could have came\n",
    "# in through .png images, but we'll show you how to do that in\n",
    "# another lab. For now, you'll see how to import .mats:\n",
    "mat = scipy.io.loadmat('Datasets/face_data.mat')\n",
    "df = pd.DataFrame(mat['images']).T\n",
    "num_images, num_pixels = df.shape\n",
    "num_pixels = int(math.sqrt(num_pixels))\n",
    "\n",
    "# Rotate the pictures, so we don't have to crane our necks:\n",
    "for i in range(num_images):\n",
    "    df.loc[i,:] = df.loc[i,:].reshape(num_pixels, num_pixels).T.reshape(-1)\n",
    "\n",
    "X = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TODO: Load up your face_labels dataset. It only has a single column, and\n",
    "# you're only interested in that single column. You will have to slice the \n",
    "# column out so that you have access to it as a \"Series\" rather than as a\n",
    "# \"Dataframe\". Use an appropriate indexer to take care of that. Also print\n",
    "# out the labels and compare to the face_labels.csv file to ensure you\n",
    "# loaded it correctly\n",
    "#\n",
    "labels = pd.read_csv('Datasets/face_labels.csv', names=['label'])\n",
    "labels = labels.iloc[:,0]\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TODO: Do train_test_split. Use the same code as on the EdX platform in the\n",
    "# reading material, but set the random_state=7 for reproduceability, and play\n",
    "# around with the test_size from 0.10 - 0.20 (10-20%). Your labels are actually\n",
    "# passed in as a series (instead of as an NDArray) so that you can access\n",
    "# their underlying indices later on. This is necessary so you can find your samples\n",
    "# in the original dataframe, which you will use to plot your testing data as images\n",
    "# rather than as points:\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Test_PCA:\n",
    "  # INFO: PCA is used *before* KNeighbors to simplify your high dimensionality\n",
    "  # image samples down to just 2 principal components! A lot of information\n",
    "  # (variance) is lost during the process, as I'm sure you can imagine. But\n",
    "  # you have to drop the dimension down to two, otherwise you wouldn't be able\n",
    "  # to visualize a 2D decision surface / boundary. In the wild, you'd probably\n",
    "  # leave in a lot more dimensions, but wouldn't need to plot the boundary;\n",
    "  # simply checking the results would suffice.\n",
    "  #\n",
    "  # Your model should only be trained (fit) against the training data (data_train)\n",
    "  # Once you've done this, you need use the model to transform both data_train\n",
    "  # and data_test from their original high-D image feature space, down to 2D\n",
    "\n",
    "  #\n",
    "  #\n",
    "  # TODO: Implement PCA here. ONLY train against your training data, but\n",
    "  # transform both your training + test data, storing the results back into\n",
    "  # data_train, and data_test.\n",
    "  #\n",
    "  # .. your code here ..\n",
    "    print('Do PCA')\n",
    "else:\n",
    "  # INFO: Isomap is used *before* KNeighbors to simplify your high dimensionality\n",
    "  # image samples down to just 2 components! A lot of information has been is\n",
    "  # lost during the process, as I'm sure you can imagine. But if you have\n",
    "  # non-linear data that can be represented on a 2D manifold, you probably will\n",
    "  # be left with a far superior dataset to use for classification. Plus by\n",
    "  # having the images in 2D space, you can plot them as well as visualize a 2D\n",
    "  # decision surface / boundary. In the wild, you'd probably leave in a lot\n",
    "  # more dimensions, but wouldn't need to plot the boundary; simply checking\n",
    "  # the results would suffice.\n",
    "  #\n",
    "  # Your model should only be trained (fit) against the training data (data_train)\n",
    "  # Once you've done this, you need use the model to transform both data_train\n",
    "  # and data_test from their original high-D image feature space, down to 2D\n",
    "\n",
    "  #\n",
    "  # TODO: Implement Isomap here. ONLY train against your training data, but\n",
    "  # transform both your training + test data, storing the results back into\n",
    "  # data_train, and data_test.\n",
    "  #\n",
    "    from sklearn.manifold import Isomap\n",
    "    iso = Isomap(n_components=2, n_neighbors=5).fit(X_train)\n",
    "    X_train = iso.transform(X_train)\n",
    "    X_test = iso.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TODO: Implement KNeighborsClassifier here. You can use any K value from 1\n",
    "# through 20, so play around with it and attempt to get good accuracy.\n",
    "# This is the heart of this assignment: Looking at the 2D points that\n",
    "# represent your images, along with a list of \"answers\" or correct class\n",
    "# labels that those 2d representations should be.\n",
    "#\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "accuracies = []\n",
    "for K in range(1,21):\n",
    "    knn = KNeighborsClassifier(n_neighbors=K)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # NOTE: K-NEIGHBORS DOES NOT CARE WHAT THE ANSWERS SHOULD BE! In fact, it\n",
    "    # just tosses that information away. All KNeighbors cares about storing is\n",
    "    # your training data (data_train) so that later on when you attempt to\n",
    "    # predict or score samples, it can derive a class for them based on the\n",
    "    # labeling of the sample's near neighbors.\n",
    "\n",
    "    #\n",
    "    # TODO: Calculate + Print the accuracy of the testing set (data_test and\n",
    "    # label_test).\n",
    "    #\n",
    "    accuracies.append(knn.score(X_test, y_test))\n",
    "\n",
    "    # Chart the combined decision boundary, the training data as 2D plots, and\n",
    "    # the testing data as small images so we can visually validate performance.\n",
    "    # Plot2DBoundary(X_train, y_train, X_test, y_test, knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9904761904761905"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9809523809523809,\n",
       " 0.9619047619047619,\n",
       " 0.9619047619047619,\n",
       " 0.9619047619047619,\n",
       " 0.9619047619047619,\n",
       " 0.9619047619047619,\n",
       " 0.9714285714285714,\n",
       " 0.9809523809523809,\n",
       " 0.9619047619047619,\n",
       " 0.9904761904761905,\n",
       " 0.9809523809523809,\n",
       " 0.9904761904761905,\n",
       " 0.9904761904761905,\n",
       " 0.9904761904761905,\n",
       " 0.9809523809523809,\n",
       " 0.9809523809523809,\n",
       " 0.9809523809523809,\n",
       " 0.9809523809523809,\n",
       " 0.9809523809523809,\n",
       " 0.9904761904761905]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
